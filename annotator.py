from openai import OpenAI
import json
from tqdm import tqdm
import multiprocessing

def prepare_prompt(i, queries, answers, answer_choice, llama_answers, mistral_answers):
    prompt = f"""You are tasked with evaluating responses from different large language models (LLMs) and selecting the best response for a given query. Your evaluation should be based on the clarity, correctness, completeness, and explanation depth of each response. Your goal is to analyze the responses and state the best response along with a justification. Analyze the answers clearly and make a decision. Below are the queries, ground truth answers, and responses generated by various LLMs."""
    
    prompt += ("""For each query, you are provided with:
        - The question
        - The possible answer choices
        - The ground truth answer (for reference, but it cannot be selected)
        - Responses from two LLMs: Llama and Mistral"""
        )
    
    prompt += (
        f"Example {i}: \n"
        f"  - Question: {queries[i]} \n"
        f"  - Choices: {answers[i]} \n"
        f"  - Ground Truth Answer: {answer_choice[i]} \n"
        f"  - Llama Answer: {llama_answers[i]} \n"
        f"  - Mistral Answer: {mistral_answers[i]} \n"
    )
    
    prompt += (
        "Selection Criteria:\n"
        "1. Understand the responses and their correlation with the given query, and rank them based on their alignment and correctness with the ground-truth answer. The ground-truth answer is only for reference and cannot be selected.\n"
        f"2. Pick the best response from the given responses (Llama and Mistral) without modifying the answers in any way. Don't add additional text wrapping the best reponse. It should exactly match with one of Llama Answer or Mistral Answer.\n"
    )
    
    prompt += ("Return the output as a structured JSON list where each example is an object containing:\n"
        f"- id: The example number (should be {i} in this case)\n"
        f"- best_response: The best response selected from Llama or Mistral\n"
        f"- llm: The name of the LLM that provided the best response (Llama or Mistral)\n"
        f"- justification: A very brief explanation of why this response was chosen\n"
    )
    
    prompt += ("Output Format:\n"
        """{\n
            "id": <query_id> ,
            "best_response": "<best_answer>",
            "llm": "<llm_name>",
            "justification": "<reasoning>"
        }"""
    )
    return prompt

def get_response(api_key, model, prompt, max_attempts=5):
    # print("Opening get response")
    client = OpenAI(api_key=api_key)
    attempts = 0
    response_object = {'responses': []}
    while attempts < max_attempts:
        state = False
        response = client.chat.completions.create(
            model=model,
            messages=[ 
                {"role": "system", "content": "You are an AI assistant designed to select the best response from the given responses."}, 
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"}
        )
        try:
            response_object = json.loads(response.choices[0].message.content.strip())
            if list(response_object.keys()) != ['id', 'best_response', 'llm', 'justification']:
                state = True
        except Exception as e:
            attempts += 1
            print("Error encountered, trying again:", e)
            continue

        if not state:
            break
        else:
            attempts += 1
            print("Malformed responseâ€”trying again")
    
    return response_object

def process_filter_batch(args):
    # print("entered here")
    api_key, model, prompt = args
    return get_response(api_key, model, prompt)

def load_json_data(filepath):
    with open(filepath, 'r') as file:
        data = json.load(file)
    return data

def save_list_to_file(response, filename):
    with open(filename, 'w') as file:
        json.dump(response, file, indent=4)

def refine():
    mmlu_data = load_json_data("./mmlu_answer_mapping.json")
    llama_data = load_json_data("./llama_4000.json")
    mistral_data = load_json_data("./mistral_4000.json")
    questions = []
    answers = []
    answer_choice = []
    llama_answers = []
    mistral_answers = []
    responses = []
    for i in range(len(mmlu_data)):
        questions.append(mmlu_data[i]['question'])
        answers.append(mmlu_data[i]['choices'])
        answer_choice.append(mmlu_data[i]['mmlu_answer'])

    for ans in llama_data:
        llama_answers.append(ans['full_response'])
    for ans in mistral_data:
        mistral_answers.append(ans['full_response'])
        
    tasks = []
    total_len = len(llama_answers)
    api_key = "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"
    for index in range(total_len):
        # print(index, int(mmlu_data[index]['query_id']))
        prompt = prepare_prompt(index, questions, answers, answer_choice, llama_answers, mistral_answers)
        tasks.append((api_key, "gpt-4o-mini", prompt))
    
    with multiprocessing.Pool(processes=2*multiprocessing.cpu_count()) as pool:
        results = list(
            tqdm(
                pool.imap_unordered(process_filter_batch, tasks),
                total=len(tasks),
                desc="Processing batches"
            )
        )
    
    for output in results:
        # print(output)
        responses.append(output)
    
    # print(responses)
    response_object = {'responses': responses}
    save_list_to_file(response_object, "./test_responses.json")
        
    return

if __name__ == "__main__":
    refine()