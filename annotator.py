from openai import OpenAI
import json
from tqdm import tqdm
import multiprocessing

def prepare_squad_prompt(i, query, context, answer, llama_answer, mistral_answer):
    prompt = f"""You are an evaluator tasked with comparing responses generated by two large language models (LLMs) for a question-answering task based on the SQuAD 2.0 dataset. Your job is to determine which response better answers the given question based on the context provided. Note that some questions may be unanswerable based on the context.

    Your evaluation should consider the following criteria:
    1. Correctness: Does the response provide a factually accurate and relevant answer based on the context?
    2. Completeness: Does the response fully address the question?
    3. Clarity: Is the response clearly worded and easy to understand?
    4. Appropriateness: Does the response refrain from hallucinating unsupported content? For unanswerable questions, does it correctly abstain from answering?

    You will be given:
    - A question
    - A context passage
    - The ground-truth answer (for reference only; not to be selected)
    - Two responses: one from Llama and one from Mistral

    Evaluate the following example:
    - Question: {query}
    - Context: {context}
    - Ground Truth Answer: {answer}
    - Llama Response: {llama_answer}
    - Mistral Response: {mistral_answer}

    Instructions:
    - Carefully compare the two responses.
    - Decide which LLM gave the better response based on the criteria above.
    - Do not rewrite or modify the responses.
    - Do not select the ground-truth answer as a response.
    - Return your decision strictly using the output format below.

    Output Format:
    {{
        "id": {i},
        "llm": <llm that gave the better response>,
        "justification": "<A short and focused explanation of why the above llm response was chosen>"
    }}
    """
    return prompt

def prepare_swag_prompt(i, ctx, ctx_a, ctx_b, activity_label, choice, llama_answer, mistral_answer):
    prompt = f"""You are an evaluator tasked with comparing responses generated by two large language models (LLMs) for a commonsense reasoning and next-sentence prediction task based on the HellaSwag dataset. Your goal is to determine which response better completes the given context in a way that is coherent, plausible, and contextually appropriate.

    Evaluation Criteria:
    1. Plausibility: Does the response follow everyday commonsense knowledge and logical progression?
    2. Relevance: Is the response a natural and sensible continuation of the given context and activity?
    3. Coherence: Is the response grammatically and semantically well-formed?

    You are provided with:
    - A context describing an activity.
    - Additional prefix components (ctx_a and ctx_b).
    - An activity label describing the scenario.
    - Four answer choices that represent valid continuations of the context.
    - Two responses generated by different models (Llama and Mistral). Each response should ideally match or align closely with one of the four answer choices.

    Example:
    - Context: {ctx}
    - ctx_a: {ctx_a}
    - ctx_b: {ctx_b}
    - Activity Label: {activity_label}
    - Choices: {choice}
    - Llama Response: {llama_answer}
    - Mistral Response: {mistral_answer}

    Instructions:
    - Compare the two model responses.
    - Determine which one more accurately, plausibly, and coherently completes the scenario.
    - Each response should ideally align with one of the provided choices, although the exact phrasing may vary.
    - Do not modify the responses.
    - Only choose between the Llama and Mistral responses.
    - Return your answer strictly in the following JSON format.

    Output Format:
    {{
        "id": {i},
        "llm": "<Llama or Mistral>",
        "justification": "<A short and clear explanation of why the selected response was better>"
    }}
    """
    return prompt


def get_response(api_key, model, prompt, keys, max_attempts=3,):
    client = OpenAI(api_key=api_key)
    attempts = 0
    response_object = None
    while attempts < max_attempts:
        state = False
        response = client.chat.completions.create(
            model=model,
            messages=[ 
                {"role": "system", "content": "You are an impartial evaluator that strictly follows instructions and returns structured JSON as requested."}, 
                {"role": "user", "content": prompt}
            ],
            temperature=0,
            max_tokens=150,
            response_format={"type": "json_object"}
        )
        try:
            response_object = json.loads(response.choices[0].message.content.strip())
            if list(response_object.keys()) != keys:
                state = True
        except Exception as e:
            attempts += 1
            print("Error encountered, trying again:", e)
            continue

        if not state:
            break
        else:
            attempts += 1
            print("Malformed responseâ€”trying again")
    
    return response_object

def process_filter_batch(args):
    api_key, model, prompt, keys = args
    return get_response(api_key, model, prompt, keys)

def load_json_data(filepath):
    with open(filepath, 'r') as file:
        data = json.load(file)
    return data

def save_list_to_file(response, filename):
    with open(filename, 'w') as file:
        json.dump(response, file, indent=4)

def refine_squad():
    squad_data = load_json_data("./combined_llama_mistral.json")
    questions = []
    answers = []
    contexts = []
    llama_answers = []
    mistral_answers = []
    responses = []
    ids = []
    for i in range(len(squad_data)):
        ids.append(squad_data[i]['id'])
        questions.append(squad_data[i]['question'])
        contexts.append(squad_data[i]['context'])
        answers.append(squad_data[i]['true_answers'])
        llama_answers.append(squad_data[i]['llama_response'])
        mistral_answers.append(squad_data[i]['mistral_response'])
        
    tasks = []
    total_len = len(ids)
    api_key = ""
    squad_keys = ["id", "llm", "justification"]
    for i in range(total_len):
        prompt = prepare_squad_prompt(i, questions[i], contexts[i], answers[i], llama_answers[i], mistral_answers[i])
        tasks.append((api_key, "gpt-4o-mini", prompt, squad_keys))
    # tasks = tasks[:5]
    with multiprocessing.Pool(processes=2*multiprocessing.cpu_count()) as pool:
        results = list(
            tqdm(
                pool.imap_unordered(process_filter_batch, tasks),
                total=len(tasks),
                desc="Processing batches"
            )
        )
    
    for output in results:
        output['chosen_model'] = output['llm'].replace("Llama", "llama").replace("Mistral", "mistral")
        if output['chosen_model'] == "llama":
            output['chosen_response'] = llama_answers[output['id']]
            output['rejected_response'] = mistral_answers[output['id']]
            output['rejected_model'] = "mistral"
        elif output['chosen_model'] == "mistral":
            output['chosen_response'] = mistral_answers[output['id']]
            output['rejected_response'] = llama_answers[output['id']]
            output['rejected_model'] = "llama"
        else:
            print("Error in model selection")
        output['question'] = questions[output['id']]
        output['context'] = contexts[output['id']]
        output['ground_truth_answer'] = answers[output['id']]
        output['id'] = ids[output['id']]
        del output['llm']
        responses.append(output)
    
    response_object = {'responses': responses}
    save_list_to_file(response_object, "./squad_ann.json")    
    return

def refine_swag():
    llama_swag = load_json_data("./llama_swag_responses.json")
    mistral_swag = load_json_data("./mistral_hellaswag_responses.json")
    total_len = len(llama_swag)
    activity_labels = [None] * total_len
    ctxs = [None] * total_len
    ctx_as = [None] * total_len
    ctx_bs = [None] * total_len
    choices = [None] * total_len
    llama_answers = [None] * total_len
    mistral_answers = [None] * total_len
    ids = []
    responses = []
    for i in range(total_len):
        try:
            assert llama_swag[i]['question_index'] == mistral_swag[i]['ind'], f"Mismatch at index {i}"
            ids.append(llama_swag[i]['question_index'])
            ctxs[i] = llama_swag[i]['context']
            ctx_as[i] = mistral_swag[i]['ctx_a']
            ctx_bs[i] = mistral_swag[i]['ctx_b']
            activity_labels[i] = llama_swag[i]['activity_label']
            choices[i] = llama_swag[i]['ending_options']
            llama_answers[i] = llama_swag[i]['llama_reasoning']
            mistral_answers[i] = mistral_swag[i]['mistral_response']
        except Exception as e:
            print(f"Error processing index {i}: {e}") 
            exit(-1)
        
    tasks = []
    api_key = ""
    swag_keys = ["id", "llm", "justification"]
    for i in range(total_len):
        prompt = prepare_swag_prompt(i, ctxs[i], ctx_as[i], ctx_bs[i], activity_labels[i], choices[i], llama_answers[i], mistral_answers[i])
        tasks.append((api_key, "gpt-4o-mini", prompt, swag_keys))
    # tasks = tasks[:5]
    with multiprocessing.Pool(processes=2*multiprocessing.cpu_count()) as pool:
        results = list(
            tqdm(
                pool.imap_unordered(process_filter_batch, tasks),
                total=len(tasks),
                desc="Processing batches"
            )
        )
    
    for output in results:
        output['chosen_model'] = output['llm'].replace("Llama", "llama").replace("Mistral", "mistral")
        if output['chosen_model'] == "llama":
            output['chosen_response'] = llama_answers[output['id']]
            output['rejected_response'] = mistral_answers[output['id']]
            output['rejected_model'] = "mistral"
        elif output['chosen_model'] == "mistral":
            output['chosen_response'] = mistral_answers[output['id']]
            output['rejected_response'] = llama_answers[output['id']]
            output['rejected_model'] = "llama"
        else:
            print("Error in model selection")
        output['ctx'] = ctxs[output['id']]
        output['ctx_a'] = ctx_as[output['id']]
        output['ctx_b'] = ctx_bs[output['id']]
        output['activity_label'] = activity_labels[output['id']]
        output['choices'] = choices[output['id']]
        del output['llm']
        responses.append(output)
    
    response_object = {'responses': responses}
    save_list_to_file(response_object, "./swag_ann.json") 
    return 

def refine_trivia():
    llama_swag = load_json_data("./evaluated_llama_responses.json")
    mistral_swag = load_json_data("./evaluated_mistral_responses.json")
    total_len = len(llama_swag)
    activity_labels = [None] * total_len
    ctxs = [None] * total_len
    ctx_as = [None] * total_len
    ctx_bs = [None] * total_len
    choices = [None] * total_len
    llama_answers = [None] * total_len
    mistral_answers = [None] * total_len
    ids = []
    responses = []
    for i in range(total_len):
        try:
            assert llama_swag[i]['question_index'] == mistral_swag[i]['ind'], f"Mismatch at index {i}"
            ids.append(llama_swag[i]['question_index'])
            ctxs[i] = llama_swag[i]['context']
            ctx_as[i] = mistral_swag[i]['ctx_a']
            ctx_bs[i] = mistral_swag[i]['ctx_b']
            activity_labels[i] = llama_swag[i]['activity_label']
            choices[i] = llama_swag[i]['ending_options']
            llama_answers[i] = llama_swag[i]['llama_reasoning']
            mistral_answers[i] = mistral_swag[i]['mistral_response']
        except Exception as e:
            print(f"Error processing index {i}: {e}") 
            exit(-1)
        
    tasks = []
    api_key = ""
    swag_keys = ["id", "llm", "justification"]
    for i in range(total_len):
        prompt = prepare_swag_prompt(i, ctxs[i], ctx_as[i], ctx_bs[i], activity_labels[i], choices[i], llama_answers[i], mistral_answers[i])
        tasks.append((api_key, "gpt-4o-mini", prompt, swag_keys))
    # tasks = tasks[:5]
    with multiprocessing.Pool(processes=2*multiprocessing.cpu_count()) as pool:
        results = list(
            tqdm(
                pool.imap_unordered(process_filter_batch, tasks),
                total=len(tasks),
                desc="Processing batches"
            )
        )
    
    for output in results:
        output['chosen_model'] = output['llm'].replace("Llama", "llama").replace("Mistral", "mistral")
        if output['chosen_model'] == "llama":
            output['chosen_response'] = llama_answers[output['id']]
            output['rejected_response'] = mistral_answers[output['id']]
            output['rejected_model'] = "mistral"
        elif output['chosen_model'] == "mistral":
            output['chosen_response'] = mistral_answers[output['id']]
            output['rejected_response'] = llama_answers[output['id']]
            output['rejected_model'] = "llama"
        else:
            print("Error in model selection")
        output['ctx'] = ctxs[output['id']]
        output['ctx_a'] = ctx_as[output['id']]
        output['ctx_b'] = ctx_bs[output['id']]
        output['activity_label'] = activity_labels[output['id']]
        output['choices'] = choices[output['id']]
        del output['llm']
        responses.append(output)
    
    response_object = {'responses': responses}
    save_list_to_file(response_object, "./swag_ann.json") 
    return 

if __name__ == "__main__":
    refine_squad()
    refine_swag()
    refine_trivia()